1. Cache vs Persistent
consider if you have rdd which you have generated by doing bunch of transformation
rdd.cache -> results are cached

2.cache and persistent have same purpose -> speed up the application
3.an rdd that is not cached is reevaluated again each time when action is invoked
4.Persistent comes with various storage levels
    i. in memory
    ii. desk
    iii. offheap

    persist() ==  cache() -> inmemory
    cache() -> if no enough storage this will be skipped so use persist instead
    do not cache or persist the base RDD -> whole purpose of cache is to save processing time
    persist(StorageLevel.DISK_ONLY) -> store to disk

    Block Eviction :
        Consider the situation that some block partitions are so large (skew) that they will
        quickly fill up storage memory used for caching

        when the storage memory becomes full, an eviction policy will be used to make up space for new blocks

        LRU (last recently used) -> is used

    Strorage Levels :

    MEMORY_ONLY -> non serialized format i.e srialiazation is  in bytes format
                 -> so takes more memory

    DISK_ONLY ->  serialized format
               -> so less memory
    MEMORY_AND_DISK -> data is cached in memory . If enough memory is not available
                        evicted blocks from memory are serialized to disk
                    -> this mode of operation is recommended when reevaluation is expensive and
                    memory resources are scarce

    OFF_HEAP -> blocks are cached of-heap
               -> save outside of the JVM
               problem with storing objects in jvm is that it used garbage collection for
               freeing up. garbage collection to free up space is a time taking process
               -> but off_heap is unsafe thing as we have to deal with raw memory outside of
               your jvm
    MEMORY_ONLY_SER -> Serialized
    MEMORY_AND_DISK_SER-> Serialized
    MEMORY_ONLY_2-> 2 indicated s replicas stored on 2 different worker Nodes
-----------------------------------------------------------------------------------------------
serialization increases the processing cost but reduces memory footprint
non serialized processing can be fat but more memory footprint

rdd.debugString -> show the lineage


-----------------------------------------------------------------------------------------------
1. Difference between lineage and DAG
2.How to create a jar for your spark project and run in cluster

Lineage : dependency graph
        -> show dependency of various rdd
        -> its a logical plan

DAG is a acyclic graph.
jobs,stages and tasks


 -----------------------------------------------------------------------
Top movies
1.1000 people should have rated for that movie
2.Average rating is greater than 4.5

-----------------------------------------------------------------------
Spark Eco system
Structred API- dataframes,datasets and spark sql
Spark streaming to process real time data
-----------------------------------------------------------------------


Spark Core
-------
rdd's

Structure API's
-----------
Dataframes and datasets

Dataframes -> structured , organized with column names
            -> equivalent to tables in Database but present in partitions
            ->richer optimizations are possible
            -> rdd's are spark1 style
            -> dataframes got greater support from spark2
            -> both are merged in spark 2 into single API Datasets API
            ->Use spark session instead of spark context

Spark Session :
            -> seperate context for each and everything earlier(spark context , hive context,sqlcontext)
             now spark session handles all those  i.e unified entry point of spark
             application
             -> it provides a way to interact with various contexts
             -> singleton object i.e one spark session for one application

number of jobs = number of actions
each job will have stages
each stage will have tasks -> number of partitions
Dataframe logic is more like SQL


1. when ever we are working with dataframes or datasets we are dealing with higher level
programming constructs..
2.Your spark compiler(driver) converts high level code to low level code(executors execute low level code) and creates the plan
-----------------------------------------------------------------------

rdd vs dataframe vs dataset

rdd -> low level code
this is not developer friendly


Dataframes
------

spark 1.3 version

higher level construct and developer friendly

challenges with dataframes
------------------------------
1. Dataframes do not offer strongly typed code ..
type errorr wont be caught at compile time rather than we get surprise at run time
2.developers felt that there flexibility has become limited

3. Dataframes can be converted to Rdd i.e df.rdd -> gives more flexibility
this conversion from dataframes to rdd is not seamless
4.if we work with raw rdd by converting dataframe to rdd . we will miss out some of major
optimization

catalyst optimizer/tungsten engine

DatasetAPI ( this address above issues)
-------------
spark 1.6

1. Compile time safety
2. We get more flexibility in terms of using lower level code
3.conversion of dataframes to dataset is seamless
4. we wont lose any of the optmimizations

before spark2 both dataframes and datasets are 2 different things

in spark2 they merged these 2 into single spark dataset API (Structured API)

Dataframe is nothing but dataset[Row]
Row is nothing but a generic type which will be bound at runtime
incase of dataframes the data types are bound at run time

Dataset[Row] -> dataframe
Dataset[Employee]-> dataset (compile time type safety)

if we replace generic row with specific objet then it becomes dataset


Dataframes are more preferred vs Datasets
conversion takes lot of time (i.e Df to Ds) since u are converting to object
Serialization -> convertinf data into binary form..
when we are dealing with dataframes then the serialization is managed by tungsten
binary format..
when we are dealing with datasets then serialization is managed by java serialization(slow)

using datasets will help us to cut down on developer mistakes but it comes with
extra cost of casting and expensive serialization

-----------------------------------------------------------------------

Structred API :
---------
1. Read the data from a data source and create a dataframe/datasets
2.external data source (mysql database,redshift,mongodb)
internal datasource(hdfs,s3,azure,blob,google,storage)
so we have flexibility in spark to create a dataframe directly from an external
datasource
spark is very good at processing but is not that efficient at ingesting data
Spark gives you a jdbc connector to ingest the data from mysql database directly
Sqoop to get the data to my hdfs and then i will load the data from hdfs to spark dataframe

use tools like sqoop which are specially made for data ingestion to get your data from external data source to internal data source

2.Performing a bunch of transformations and actions
transformations/actions using higher level constructs

3.writing the data to the target(Sink)
internal/external

-----------------------------------------------------------------------
3 read modes
1. Permissive (it sets all the fields to null when it encounters a corrupted reecord) ->Default
2.DROPMALFORMED (will ignore the malformed records )
3. FAILFAST (when ever a malformed record sis encountered a exception is raised)